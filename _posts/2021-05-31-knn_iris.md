---
layout:     post
title:      K-Nearest Neighbor algorithm on Iris dataset
subtitle:   Workflow of a basic classification task
date:       2021-05-29
author:     Martin Beneš
header-img: img/knn_iris_files/Iris_versicolor.jpg
catalog: true
katex: true
tags:
    - machinelearning
    - sklearn
    - classification
    - knn
    - crossvalidation
    - visualization
    - iris
---

Let's go through a basic steps of a classification workflow on species of plants in popular dataset *iris*. For implementation we shall use Python and a library *scikit-learn* (`sklearn`).

To score the accuracy we will see a holdout and a cross-validation.

## Data

The dataset `iris` is very popular amongst machine learners in example tasks such as this one. For this reason it is also present directly in the `sklearn` package.


```python
# load iris dataset

from sklearn.datasets import load_iris
iris = load_iris()
```

In the dataset there are three different species of genus Iris: I. setosa, I. versicolor and I. virginica. We are given 150 plants (50 per species), where for each of the samples we measured width and length of petal and sepal of the flower.

<img src="/img/knn_iris_files/petal_sepal.png" width = "50%"/>


```python
print(iris.data.shape, iris.target.shape) # iris data and labels

print(iris.target_names) # names of labels

print(iris.feature_names) # names of attributes
```

    (150, 4) (150,)
    ['setosa' 'versicolor' 'virginica']
    ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
    

The task will be to determine the species for a sample based on the dimensions of its petals and sepals.


```python
# plot group distribution

import pandas as pd
import matplotlib.pyplot as plt
target_str = pd.Series(iris.target).apply(lambda i: iris.target_names[i])
ax = target_str.hist()
```


![svg](/img/knn_iris_files/knn_iris_8_0.svg)


Fortunately `iris` dataset is balanced, it has the same number of samples for each species, as you can see in the bar chart above, Balanced dataset makes it simple to proceed directly to the classification phase.

In opposite case we would have to do additional steps to reduce the negative effects. Such problem is called *imbalanced classification*.


```python
# iris as data frame

iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
# create a scatter matrix from the dataframe, color by target

ax = pd.plotting.scatter_matrix(iris_df, c=iris.target,
    figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, s=60, alpha=.8)
```


![svg](/img/knn_iris_files/knn_iris_10_0.svg)


In the plot we can see domains of values as well as the distributions of each of the attribute. It is also possible to compare groups in scatter plots over all pairs of attributes. From those it seems that groups are well separated, two of the groups slightly overlap.

## Train-test split

The dataset is split onto train and test sets in ratio 75:25.

* Train set is used for training.
* Test set is used for estimation of model performance to find out, whether model generalizes well.

This method is sometimes also called *holdout*.



```python
# train test split

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(iris.data, iris.target, train_size=.75, random_state=12345)

print(xtrain.shape, ytrain.shape) # dimension of train set

print(xtest.shape, ytest.shape) # dimension of test set
```

    (112, 4) (112,)
    (38, 4) (38,)
    

## kNN model

Intuitively the idea of the kNN (k-nearest neighbors) model is 

> Majority of the most similar samples are class A, so the sample is also class A.

The black point in the plot below would be classified as species versicolor.

<img src="/img/knn_iris_files/knn_points.png" />

However, with raised eyebrow we should be asking following questions now.

* How do you define "similar samples"?
* How many of these similar samples should we take?

Fortunately the math of the model is implemented and hidden inside the package `sklearn`, so we do not have to reimplement it, just understand, what it does.

In the plot before, we are using **Euclidean metric** *as crow flies*, which is intuitive. Its mathematical formula is

$$d(x,y) = \sqrt{(x[1]-y[1])^2 + (x[2]-y[2])^2 + ... } = \sqrt{\sum_{i=1}^D(x[i]-y[i])^2}$$

However there are cases, when we might need something else:

* Many cities (especially in US, such as New York City) have orthogonal grid of streets. If you travel from A to B, you travel separately in x and y. Such distance metric is called **Manhattan metric**.

$$d(x,y) = (x[1]-y[1]) + (x[2]-y[2]) + ... = \sum_{i=1}^D(x[i]-y[i])$$

* Similarity of geographic coordinates is done with spheric distance, rather than planear. Most common metric is **great-circle distance**, usually computed with *haversine formula*.

$$g = (\text{longitude}~\lambda_g, \text{latitude}~\phi_g)$$

$$d(x,y) = 2~\text{arcsin}\sqrt{\text{sin}^2\Big(\frac{\Delta\phi}{2}\Big) + \cos{(x_\phi)}\cos{(y_\phi)}\text{sin}^2\Big(\frac{\Delta\lambda}{2}\Big)}$$

<!--**Similar samples** have similar values in its attributes. Imagine two points in a plane: the closer they are, more similar their x and y coordinates are. And vice versa, further the points are from each other, more different their x and y coordinates become. Similarity is the inverse of distance.

In kNN we work with minimal distance. Selection of how to compute distance (*distance metric*) is not so trivial, the most intuitive and the one we shall use is the "as bird flies", called **Euclidean distance**. Its mathematical equation is-->



All the examples above were done with two dimensions, but keep in mind that in our task we operate in 4-dimensional space - our data attributes. With different dataset it can be much more.

Selection of the **number of similar samples** will be discussed later in the section [Hyperparameter tuning](#hyperparameter-tuning). For now we shall use 5.

## Train the model

We define kNN classifier with class `KNeighborsClassifier`, hyperparameter `n_neighbors` value is 5.


```python
# create a knn, k = 5

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
```

For training the model only train set is used.


```python
# fit to the train set

knn.fit(xtrain, ytrain)
```

Classifier is trained now, information about the train data has been extracted and save inside the object `knn`. Method `fit()` is present in all the *estimators* of the package `scikit-learn`. Documentation defines estimator as

> An object that fits a model based on some training data and is capable of inferring some properties on new data. It can be, for instance, a classifier or a regressor. All estimators implement the `fit()` method.

The method `fit()` works *in situ* (latin for *in place*), i.e. changes the inner state of the object.

## Predict

Fitted model is now able to receive the input data and produce predictions of the labels. We will predict labels for both train and test data.


```python
ytrain_pred = knn.predict(xtrain) # predict on train set

ytest_pred = knn.predict(xtest) # predict on test set
```

## Evaluation

Comparison of predicted and true label can tell us valuable information about how well our model performs. The simplest performance statistic is the ratio of correct predictions, called *accuracy*.

$$\text{accuracy} = \frac{\#~\text{correct predictions}}{\#~\text{predictions}}$$


```python
# accuracy

acc_train = (ytrain == ytrain_pred).mean()
acc_test = (ytest == ytest_pred).mean()
print(f"Train set: {acc_train}")
print(f"Test set: {acc_test}")
```

    Train set: 0.9642857142857143
    Test set: 0.9736842105263158
    

The accuracy of the classifier on the train set should be the same or a bit better than on the test set. If the test accuracy is much worse, we are overfitting the model and have to change parameters. In kNN, the value of `n_neighbors` is the key to achieve the best results, more about it in the section [Hyperparameter tuning](#hyperparameter-tuning).

More complex method to evaluate the performance of a classifier is constructing a *confusion matrix*, that shows not only accuracies for each of the classes (labels), but what classes is the classifier most confused about.


```python
# confusion matrix

from sklearn.metrics import confusion_matrix,plot_confusion_matrix
cM_train = confusion_matrix(ytrain, ytrain_pred)
cM_train
```




    array([[37,  0,  0],
           [ 0, 34,  2],
           [ 0,  2, 37]], dtype=int64)



On the train set, two samples of versicolor were classified as virginica and two samples of virginica as versicolor. All the setosa samples were classified correctly.

The accuracy of the prediction can be derived from the confusion matrix as sum of the matrix diagonal over sum of whole matrix.


```python
import numpy as np
np.diag(cM_train).sum() / cM_train.sum()
```




    0.9642857142857143



Confusion matrix can be used to get other interesting statistics about the prediction, such as sensitivity or true positive rate (TPR), specificity or true negative rate (TNR), positive predictive value (PPV) and negative predictive value (NPV). More about these in another article.

We can also visualize confusion matrix in form of heatmap.


```python
# plot confusion matrix as heatmap

from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt
plot_confusion_matrix(knn, xtest, ytest, display_labels=iris.target_names)
plt.show()
```


![svg](/img/knn_iris_files/knn_iris_38_0.svg)


On the test set, classifier worked flawlessly on setosas and virginicas. One versicolor sample was mistakenly classified as virginica.

## Hyperparameter tuning

**TODO**


```python
# create a knn

from sklearn.neighbors import KNeighborsClassifier
knn2 = KNeighborsClassifier()
```

First method used to look for the best value of `n_neighbors` is *randomized search*.


```python
# tune hyperparameter k

from sklearn.model_selection import RandomizedSearchCV
distributions = dict(n_neighbors=list(range(1,50)))
clf = RandomizedSearchCV(knn2, distributions, random_state=0, n_iter = 10)
# run 
search = clf.fit(xtrain, ytrain)
```


```python
search.best_params_['n_neighbors']
```




    5



Second method that finds the optimal value of `n_neighbors` parameters is *grid search*.


```python
# tune hyperparameter k

from sklearn.model_selection import GridSearchCV
parameters = dict(n_neighbors=list(range(1,50)))
clf2 = GridSearchCV(knn2, parameters)
# run 

search2 = clf2.fit(xtrain, ytrain)
search2.best_params_['n_neighbors']
```




    3



Hyperparameters are not trained during the training, but the accuracy is dependent on it.


```python
# accuracy on train and test sets over parameter values 
acc_train,acc_test = [],[]
for k in range(1,100):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(xtrain, ytrain)
    acc_train.append((ytrain == knn.predict(xtrain)).mean())
    acc_test.append((ytest == knn.predict(xtest)).mean())
```


```python
# plot accuracies over n_neighbors

plt.plot(acc_train, label='Train accuracy')
plt.plot(acc_test, label='Test accuracy')
plt.legend()
plt.show()
```


![svg](/img/knn_iris_files/knn_iris_50_0.svg)


## References

- Andreas C. Müller & Sarah Guido: Introduction to Machine Learning with Python (2017). ISBN: <a href="https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/">978-1-449-36941-5</a>.
- Jake VanderPlas: In Depth: Principal Component Analysis. <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">https://jakevdp.github.io/</a>
- Tzu-Chi Lin: Day 3 - K-Nearest Neighbors and Bias-Variance Tradeoff. <a href="https://medium.com/30-days-of-machine-learning/day-3-k-nearest-neighbors-and-bias-variance-tradeoff-75f84d515bdb">https://medium.com/</a>
- Scikit-learn: Machine Learning in Python. <a href="https://scikit-learn.org/">https://scikit-learn.org/</a>
- Danielle Langlois: Iris versicolor photo. <a href="https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg">Iris versicolor 3.jpg</a>
- Black And White Flower. <a href="https://www.kissclipart.com/parts-of-a-flower-for-class-6-clipart-flower-petal-zrjj42/">https://www.kissclipart.com/</a>
