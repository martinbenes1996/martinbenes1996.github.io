---
layout:     post
title:      K-Nearest Neighbor algorithm on Iris dataset
subtitle:   Workflow of a basic classification task
date:       2021-05-29
author:     Martin Beneš
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - machinelearning
    - sklearn
    - classification
    - knn
    - crossvalidation
    - visualization
    - iris
---

Let's go through a basic steps of a classification workflow on species of plants in popular dataset *iris*. For implementation we shall use Python and a library *scikit-learn* (`sklearn`).

To score the accuracy we will see a holdout and a cross-validation.

## Data

Iris dataset is very popular dataset and is also present directly in the `sklearn` package.


```python
# load iris dataset

from sklearn.datasets import load_iris
iris = load_iris()
# print summary

print("Data:", iris.data.shape)
print("Labels:", iris.target.shape)
print("Groups:", iris.target_names)
print("Attributes:", iris.feature_names)
```

    Data: (150, 4)
    Labels: (150,)
    Groups: ['setosa' 'versicolor' 'virginica']
    Attributes: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
    


```python
# plot group distribution
import matplotlib.pyplot as plt
target_str = pd.Series(iris.target).apply(lambda i: iris.target_names[i])
ax = target_str.hist()
```


![png](/img/knn_iris_files/knn_iris_5_0.png)


Dataset contains $150$ samples of $3$ targets / groups, Iris species, namely Iris setosa, Iris versicolor and Iris virginica. Dataset is balanced, sizes of groups are the same ($50:50:50$).

Samples have 4 attributes / dimensions, measurements of lengths and widths of their sepals and petals in centimeters.

<img src="/img/petal_sepal.png" width = "50%"/>

<!--![Scheme of flower](/images/petal_sepal.png)-->


```python
# iris as data frame

iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
# create a scatter matrix from the dataframe, color by target

ax = pd.plotting.scatter_matrix(iris_df, c=iris.target,
    figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, s=60, alpha=.8)
```


![png](/img/knn_iris_files/knn_iris_7_0.png)


In the plot we can see domains of values as well as the distributions of each of the attribute. It is also possible to compare groups in scatter plots over all pairs of attributes. From those it seems that groups are well separated, two of the groups slightly overlap.

## Train-test split

The dataset is split onto train and test sets in ratio $75:25$.

* Train set is used for training.
* Test set is used for estimation of model performance to find out, whether model generalizes well.

This method is sometimes also called *holdout*.



```python
# train test split

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(iris.data, iris.target, train_size=.75, random_state=12345)
```


```python
print("Train set:", xtrain.shape, ytrain.shape)
print("Test set:", xtest.shape, ytest.shape)
```

    Train set: (112, 4) (112,)
    Test set: (38, 4) (38,)
    

## Train a model

The classifier 


```python
# create a knn, k = 5

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
```


```python
# fit to the train set

knn.fit(xtrain, ytrain)
```




    KNeighborsClassifier()



## Predict


```python
# predict on train set

ytrain_pred = knn.predict(xtrain)
# predict on test set

ytest_pred = knn.predict(xtest)
```

## Evaluation


```python
# accuracy

acc_train = (ytrain == ytrain_pred).mean()
acc_test = (ytest == ytest_pred).mean()
print(f"Train set: {acc_train}")
print(f"Test set: {acc_test}")
```

    Train set: 0.9642857142857143
    Test set: 0.9736842105263158
    


```python
# confusion matrix

from sklearn.metrics import confusion_matrix,plot_confusion_matrix
cf = confusion_matrix(ytrain, ytrain_pred)
# plot confusion matrix as heatmap

import matplotlib.pyplot as plt
plot_confusion_matrix(knn, xtest, ytest, display_labels=iris.target_names)
plt.show()
```


![png](/img/knn_iris_files/knn_iris_21_0.png)


## Hyperparameter tuning


```python
# create a knn

from sklearn.neighbors import KNeighborsClassifier
knn2 = KNeighborsClassifier()
```

First method used to look for the best value of `n_neighbors` is *randomized search*.


```python
# tune hyperparameter k

from sklearn.model_selection import RandomizedSearchCV
distributions = dict(n_neighbors=list(range(1,50)))
clf = RandomizedSearchCV(knn2, distributions, random_state=0, n_iter = 10)
# run 
search = clf.fit(xtrain, ytrain)
```


```python
search.best_params_['n_neighbors']
```




    5



Second method that finds the optimal value of `n_neighbors` parameters is *grid search*.


```python
# tune hyperparameter k

from sklearn.model_selection import GridSearchCV
parameters = dict(n_neighbors=list(range(1,50)))
clf2 = GridSearchCV(knn2, parameters)
# run 

search2 = clf2.fit(xtrain, ytrain)
```


```python
search2.best_params_['n_neighbors']
```




    3



Hyperparameters are not trained during the training, but the accuracy is dependent on it.


```python
# accuracy on train and test sets over parameter values

acc_train,acc_test = [],[]
for k in range(1,100):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(xtrain, ytrain)
    acc_train.append((ytrain == knn.predict(xtrain)).mean())
    acc_test.append((ytest == knn.predict(xtest)).mean())
```


```python
plt.plot(acc_train, label='Train accuracy')
plt.plot(acc_test, label='Test accuracy')
plt.legend()
plt.show()
```


![png](/img/knn_iris_files/knn_iris_32_0.png)


## References

- Andreas C. Müller & Sarah Guido: Introduction to Machine Learning with Python (2017). ISBN: <a href="https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/">978-1-449-36941-5</a>.
- Scikit-learn: Machine Learning in Python. <a href="https://scikit-learn.org/">https://scikit-learn.org/</a>
- Black And White Flower. <a href="https://www.kissclipart.com/parts-of-a-flower-for-class-6-clipart-flower-petal-zrjj42/">https://www.kissclipart.com/</a>

